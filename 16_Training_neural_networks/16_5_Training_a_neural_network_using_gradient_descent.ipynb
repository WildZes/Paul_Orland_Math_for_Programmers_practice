{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8U5WKYGJhtg"
      },
      "outputs": [],
      "source": [
        "# Exercise 16.11\n",
        "def test_digit_classify(classifier, start=0, stop=1000):\n",
        "  correct = 0\n",
        "  for img, target in zip(digits.images[start:stop], digits.target[start:stop]):\n",
        "    v = np.matrix.flatten(img)/15.\n",
        "    output = classifier(v)\n",
        "    if output == target:\n",
        "      correct += 1\n",
        "  return (correct/(stop-start))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "digits = datasets.load_digits()\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(16,),\n",
        "                    activation='logistic',\n",
        "                    max_iter=100,\n",
        "                    verbose=25,\n",
        "                    random_state=1,\n",
        "                    learning_rate_init=.1)\n",
        "\n",
        "x = np.array([np.matrix.flatten(img) for img in digits.images[:1000]]) / 15.0\n",
        "y = digits.target[:1000]\n",
        "\n",
        "mlp.fit(x,y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSqBjF_SL1h7",
        "outputId": "5af46c48-6ab0-4df3-ee03-af9b7bd494c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 2.21958598\n",
            "Iteration 2, loss = 1.56912978\n",
            "Iteration 3, loss = 0.98970277\n",
            "Iteration 4, loss = 0.57473464\n",
            "Iteration 5, loss = 0.34048448\n",
            "Iteration 6, loss = 0.21495855\n",
            "Iteration 7, loss = 0.14366771\n",
            "Iteration 8, loss = 0.11077020\n",
            "Iteration 9, loss = 0.08764273\n",
            "Iteration 10, loss = 0.07193546\n",
            "Iteration 11, loss = 0.06020348\n",
            "Iteration 12, loss = 0.04961899\n",
            "Iteration 13, loss = 0.03979645\n",
            "Iteration 14, loss = 0.03334502\n",
            "Iteration 15, loss = 0.02996006\n",
            "Iteration 16, loss = 0.02603968\n",
            "Iteration 17, loss = 0.02355514\n",
            "Iteration 18, loss = 0.02137348\n",
            "Iteration 19, loss = 0.01967878\n",
            "Iteration 20, loss = 0.01751214\n",
            "Iteration 21, loss = 0.01617330\n",
            "Iteration 22, loss = 0.01460386\n",
            "Iteration 23, loss = 0.01408517\n",
            "Iteration 24, loss = 0.01270504\n",
            "Iteration 25, loss = 0.01191634\n",
            "Iteration 26, loss = 0.01114222\n",
            "Iteration 27, loss = 0.01045989\n",
            "Iteration 28, loss = 0.00983648\n",
            "Iteration 29, loss = 0.00920912\n",
            "Iteration 30, loss = 0.00890851\n",
            "Iteration 31, loss = 0.00843426\n",
            "Iteration 32, loss = 0.00796039\n",
            "Iteration 33, loss = 0.00749839\n",
            "Iteration 34, loss = 0.00726271\n",
            "Iteration 35, loss = 0.00673963\n",
            "Iteration 36, loss = 0.00655405\n",
            "Iteration 37, loss = 0.00626207\n",
            "Iteration 38, loss = 0.00600639\n",
            "Iteration 39, loss = 0.00581857\n",
            "Iteration 40, loss = 0.00557529\n",
            "Iteration 41, loss = 0.00533573\n",
            "Iteration 42, loss = 0.00519479\n",
            "Iteration 43, loss = 0.00505128\n",
            "Iteration 44, loss = 0.00490121\n",
            "Iteration 45, loss = 0.00469161\n",
            "Iteration 46, loss = 0.00459590\n",
            "Iteration 47, loss = 0.00464844\n",
            "Iteration 48, loss = 0.00445157\n",
            "Iteration 49, loss = 0.00425515\n",
            "Iteration 50, loss = 0.00424934\n",
            "Iteration 51, loss = 0.00397800\n",
            "Iteration 52, loss = 0.00399927\n",
            "Iteration 53, loss = 0.00383932\n",
            "Iteration 54, loss = 0.00372439\n",
            "Iteration 55, loss = 0.00361744\n",
            "Iteration 56, loss = 0.00356447\n",
            "Iteration 57, loss = 0.00345899\n",
            "Iteration 58, loss = 0.00336792\n",
            "Iteration 59, loss = 0.00330330\n",
            "Iteration 60, loss = 0.00321734\n",
            "Iteration 61, loss = 0.00315784\n",
            "Iteration 62, loss = 0.00309975\n",
            "Iteration 63, loss = 0.00303268\n",
            "Iteration 64, loss = 0.00298242\n",
            "Iteration 65, loss = 0.00294143\n",
            "Iteration 66, loss = 0.00288273\n",
            "Iteration 67, loss = 0.00282763\n",
            "Iteration 68, loss = 0.00277049\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='logistic', hidden_layer_sizes=(16,),\n",
              "              learning_rate_init=0.1, max_iter=100, random_state=1, verbose=25)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sklearn_trained_classify(v):\n",
        "  return mlp.predict([v])[0]\n",
        "\n",
        "test_digit_classify(sklearn_trained_classify, 1000, 1500), test_digit_classify(sklearn_trained_classify, 1500, 2000), test_digit_classify(sklearn_trained_classify, 1000, 2000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHH0A-_MMfA7",
        "outputId": "483f7d9c-f259-4081-926c-a413d2d94e9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.962, 0.534, 0.748)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution from the book\n",
        "def make_it_work(v):\n",
        "  return mlp.predict_proba(v.reshape(1,-1))[0]\n",
        "\n",
        "def test_digit_classify(classifier,start=0,test_count=1000):\n",
        "  correct = 0\n",
        "  end = start + test_count\n",
        "  for img, target in zip(digits.images[start:end], digits.target[start:end]):\n",
        "    v = np.matrix.flatten(img) / 15\n",
        "    output = classifier(v)\n",
        "    answer = list(output).index(max(output))\n",
        "    if answer == target:\n",
        "      correct += 1\n",
        "  return (correct/test_count)"
      ],
      "metadata": {
        "id": "GtsFlxiCM7RR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_digit_classify(make_it_work,start=1000,test_count=500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxD60ZSvO6cb",
        "outputId": "8587d469-691a-434b-9b5b-76215eabb42e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.962"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "v = np.matrix.flatten(digits.images[0]) / 15.\n",
        "list(mlp.predict_proba(v.reshape(1,-1))[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwhZkh71RRX6",
        "outputId": "add6667d-3d40-4e7b-8fe0-72fe11af7de4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9998096440320993,\n",
              " 4.987539637320318e-11,\n",
              " 2.929654240018928e-06,\n",
              " 1.1375937258183889e-07,\n",
              " 1.6524472418663667e-06,\n",
              " 2.7804078486477127e-05,\n",
              " 5.377961391146741e-06,\n",
              " 9.190047198209276e-05,\n",
              " 8.287621962335703e-08,\n",
              " 6.0494669091354046e-05]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 16.13â€”Mini Project\n",
        "# mlp.coefs_[0] @ \n",
        "layer_sizes = [64,16,10]\n",
        "weights = [np.random.rand(n,m) for m,n in zip(layer_sizes[:-1], layer_sizes[1:])]\n",
        "biases = [np.random.rand(n) for n in layer_sizes[1:]]\n",
        "v.reshape(1,-1).shape, weights[0].shape, mlp.coefs_[0].shape, biases[0].shape, mlp.intercepts_[0].shape, mlp.coefs_[0].T.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oa6Bo0-GTbs8",
        "outputId": "f3b33de2-b839-4fc1-e689-5887719270d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1, 64), (16, 64), (64, 16), (16,), (16,), (16, 64))"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigm(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "for w, b in zip(mlp.coefs_, mlp.intercepts_):\n",
        "  print(w.shape, b.shape)\n",
        "tmp = v.reshape(1,-1) @ mlp.coefs_[0] + mlp.intercepts_[0]\n",
        "tmp.shape, np.array([sigm(x) for x in tmp]).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDKgradbczsi",
        "outputId": "cb0397c7-9925-4c21-d47b-a3967fdbae9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 16) (16,)\n",
            "(16, 10) (10,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1, 16), (1, 16))"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP():\n",
        "  def __init__(self,layer_sizes, weights, biases):\n",
        "    self.layer_sizes = layer_sizes\n",
        "    self.weights = weights\n",
        "    self.biases = biases\n",
        "\n",
        "  def feedforward(self,v):\n",
        "    activations = []\n",
        "    a = v\n",
        "    activations.append(a)\n",
        "    a = v.reshape(1,-1)\n",
        "    for w,b in zip(self.weights, self.biases):\n",
        "      z = a @ w + b\n",
        "      a = np.array([sigm(x) for x in z])\n",
        "      activations.append(a)\n",
        "    return activations\n",
        "\n",
        "  def evaluate(self,v):\n",
        "    return np.array(self.feedforward(v)[-1])[0]"
      ],
      "metadata": {
        "id": "JJv4lbMuVnql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn = MLP([64,16,10], mlp.coefs_, mlp.intercepts_)\n",
        "v = np.matrix.flatten(digits.images[0]) / 15.\n",
        "nn.evaluate(v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJVtUPEZW6F2",
        "outputId": "7c72632d-4e7b-4aec-b235-1cb9081069dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9.99979768e-01, 2.46558256e-06, 1.26505855e-01, 5.59224195e-03,\n",
              "       7.55195824e-02, 5.78857597e-01, 2.10022820e-01, 8.19595433e-01,\n",
              "       4.08026651e-03, 7.49408105e-01])"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution from the book\n",
        "import math\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + math.exp(-x))\n",
        "\n",
        "\n",
        "class MLP():\n",
        "  def __init__(self,layer_sizes):\n",
        "    self.layer_sizes = layer_sizes\n",
        "    self.weights = [np.random.rand(n,m) for m,n in zip(layer_sizes[:-1], layer_sizes[1:])]\n",
        "    self.biases = [np.random.rand(n) for n in layer_sizes[1:]]\n",
        "\n",
        "  def feedforward(self,v):\n",
        "    activations = []\n",
        "    a = v\n",
        "    activations.append(a)\n",
        "    for w,b in zip(self.weights, self.biases):\n",
        "      z = w @ a + b\n",
        "      a = [sigmoid(x) for x in z]\n",
        "      activations.append(a)\n",
        "    return activations\n",
        "\n",
        "  def evaluate(self,v):\n",
        "    return np.array(self.feedforward(v)[-1])\n",
        "\n",
        "\n",
        "nn = MLP([64,16,10])\n",
        "nn.weights = [w.T for w in mlp.coefs_]\n",
        "nn.biases = mlp.intercepts_\n",
        "test_digit_classify(nn.evaluate, start=1000, test_count=500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXnrYXw9XzFw",
        "outputId": "390a84dc-6103-4feb-b222-a68f5f1f72f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.962"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xss9phl7jf7H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}